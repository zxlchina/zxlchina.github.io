---
layout: post
title:  "基于Spark做欧洲杯结果预测"
date:   2016-07-10 22:26:28 +0800
categories: AI predict
---

欧洲杯今晚就决赛了，前段时间参加一个欧洲杯的比赛，通过AI预测欧洲杯，本文即是对本次比赛所使用的方法做的总结。

本文适用于刚接触AI的小白，大神请绕行。全文分为如下几个部分

* Spark环境搭建

* 特征选择

* 模型选择

* 预测结果



# 一、Spark环境搭建

## 安装Hadoop

Spark的运行依赖Hadoop，访问[http://www.apache.org](http://www.apache.org)，选择一个binary包
[hadoop-2.6.4.tar.gz](http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz)，解压后修改如下配置：<br>

* etc/hadoop/core-site.xml


{% highlight xml %}
<?xml version="1.0" encoding="UTF-8"?>                                                                      
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/Users/lichzhang/ApplicationsLich/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
{% endhighlight %}

* etc/hadoop/hdfs-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/Users/lichzhang/ApplicationsLich/hadoop/tmp/dfs/name</value>
    </property>

    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/Users/lichzhang/ApplicationsLich/hadoop/tmp/dfs/data</value>
    </property>
</configuration>
```

* 启动hadoop

```
./sbin/start-all.sh 
```

* 验证hadoop正常运行

```bash
hadoop fs -mkdir test #创建hdfs目录
hadoop fs -ls   #查看目录
```

## 安装Spark

下载Spark的binary包[spark-1.6.2-bin-hadoop2.6.tgz](http://www.apache.org/dyn/closer.lua/spark/spark-1.6.2/spark-1.6.2-bin-hadoop2.6.tgz)，解压后编辑如下配置文件

* conf/spark-env.sh

```bash
cp conf/spark-env.sh.template conf/spark-env.sh
vim conf/spark-env.sh
```
编辑文件如下：

```bash
export SPARK_MASTER_IP=127.0.0.1
export SPARK_MASTER_PORT=7077
export MASTER=spark://${SPARK_MASTER_IP}:${SPARK_MASTER_PORT}
export SPARK_WORKER_CORES=1
export SPARK_LOCAL_IP=127.0.0.1
```

* 启动Spark

```
./sbin/start-all.sh
```

* 运行例子

键入如下命令，即可运行自带的计算PI的例子：

```
./bin/run-example SparkPi
```
运行结果如下：
```
Pi is roughly 3.14326
```


# 二、特征选择
足球是变数非常大的一项运动，比赛的胜负和太多因素有关，譬如球员的身体状况、情绪因素、天气、主客场、草地状况等等等等，获取各类数据并非易事，用好数据也是需要费一番心思，这里选择了最容易拿到的数据，具体如下：

* UEFA队伍排名信息、进球数、净胜球数

* 最近517场比赛结果数据，包括主、客场，比分数据

* 各大机构的赔率信息 

具体格式如下：

得分信息 主场队伍信息 客场队伍信息 赔率信息

# 三、模型选择
比赛预测方式比较多，譬如可以通过预测比分来的到胜负关系，也可以直接预测胜负平，预测的方式可以是回归模型，也可以是分类模型。
这里采用预测进球数的方式来预测比赛结果，具体来说，分别预测一场比赛的主、客场进球数，的到最终比分。
模型分别采用了回归和分类两类模型，具体有Linear Regression、Ridge Regression、Lasso、Logistic Regression。

* 回归模型

通过各个特征和最重得分，得到自变量（特征）和因变量（进球数）的函数关系，并用该函数关系去预测新的比赛。

* 分类模型

分类模型将主场、客场得分分类到0～11分，通过历史比赛训练分类器，然后再用分类器对新的比赛进行分类，得到比赛分


# 四、预测结果
从预测的误差来看，分类模型会优于回归模型，最后采用了逻辑回归，对八强队伍预测对了五个，效果不是很好。分析其原因，可能使特征使用的太少，对队伍的刻画不全面，同时赔率信息对结果的影响太大，发现很多场比赛的预测结果是和赔率信息相一致，究其原因也是特征选择上的问题。个人觉得，和特征相比，模型显得不是那么重要，同时借助Spark平台也是可以非常方便尝试各种模型，基本没有什么成本，所以各个队伍的差距大多数体现在特征选取和使用上。


最后附上代码


{% highlight java %}
import java.util.Arrays;

import org.apache.commons.math3.geometry.Vector;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.ml.regression.LinearRegressionModel;
import org.apache.spark.mllib.classification.LogisticRegressionModel;
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS;
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD;
import org.apache.spark.mllib.classification.SVMModel;
import org.apache.spark.mllib.classification.SVMWithSGD;
import org.apache.spark.mllib.linalg.Vectors;
import org.apache.spark.mllib.linalg.distributed.RowMatrix;
import org.apache.spark.mllib.regression.GeneralizedLinearModel;
import org.apache.spark.mllib.regression.LabeledPoint;
import org.apache.spark.mllib.regression.LassoModel;
import org.apache.spark.mllib.regression.LassoWithSGD;
import org.apache.spark.mllib.regression.LinearRegressionWithSGD;
import org.apache.spark.mllib.regression.RidgeRegressionModel;
import org.apache.spark.mllib.regression.RidgeRegressionWithSGD;

import scala.Tuple2;

public class Sulotion {

    
    private static Object resizeArray (Object oldArray, int newSize) {
           int oldSize = java.lang.reflect.Array.getLength(oldArray);
           Class elementType = oldArray.getClass().getComponentType();
           Object newArray = java.lang.reflect.Array.newInstance(
                 elementType, newSize);
           int preserveLength = Math.min(oldSize, newSize);
           if (preserveLength > 0)
              System.arraycopy(oldArray, 0, newArray, 0, preserveLength);
           return newArray; 
           }
    
    
    public static void main(String[] args) {
        // TODO Auto-generated method stub
        System.out.println("Enter Main");
        if (args.length < 3)
        {
            System.out.println("Args: DataFile StepCount TrainRatio [PredictFile] [Step]");
            return ;
        }
        
        String datafile = args[0];
        int numIterations = Integer.parseInt(args[1]); // 迭代次数
        double train_ratio = Double.parseDouble(args[2]); //训练集比例
        final int col_count = 63;//15;  //训练列数量
        
        System.out.println("DataFile:" + datafile);
        
        String predict_filename = null;
        if (args.length >= 4)
        {
            //需要预测
            predict_filename = args[3];
            System.out.println("PredictFile:" + predict_filename);
        }
        System.out.println("StepCount:" + numIterations);
        System.out.println("TrainRatio:" + train_ratio);
        
        SparkConf sparkConf = new SparkConf().setAppName("EuropeCup");
        JavaSparkContext sc = new JavaSparkContext(sparkConf);
        JavaRDD<String> data = sc.textFile(datafile);
        JavaRDD<LabeledPoint> allData = data.map(
                line -> {
                    String[] parts = line.split(",");
                    double[] ds = Arrays.stream(parts[1].split(" "))
                            .mapToDouble(Double::parseDouble).toArray();
                    double[] simple_ds = (double[])resizeArray(ds, col_count);
                    return new LabeledPoint(Double.parseDouble(parts[0]),
                            Vectors.dense(simple_ds));
                }).cache();
        
        JavaRDD<LabeledPoint> trainData = allData.sample(false, train_ratio, 11L);
        JavaRDD<LabeledPoint> testData = allData;//.subtract(trainData);
        
        //输出集合相关信息
        trainData.saveAsTextFile(predict_filename + "_train");
        double step = 0.01;
        if (args.length >= 5)
        {
            step = Double.parseDouble(args[4]);
        }
        
        
        System.out.println("");
        System.out.println("");
        System.out.println("Train Columns:" + (allData.collect().get(0)).features().size());
        System.out.println("Train Count:" + trainData.count());
        System.out.println("Test Count:" + testData.count());
        System.out.println("Step:" + step);
        System.out.println("");
        System.out.println(""); 
        
        
        //线性回归模型
        org.apache.spark.mllib.regression.LinearRegressionModel model = LinearRegressionWithSGD
                .train(trainData.rdd(), numIterations, step);
        //岭回归模型
        RidgeRegressionModel model1 = RidgeRegressionWithSGD.train(
                trainData.rdd(), numIterations, step, 0.01);
        //Lasso模型
        LassoModel model2 = LassoWithSGD.train(trainData.rdd(), numIterations, step, 0.01);
        //逻辑回归（分类）
        LogisticRegressionModel model3 = new LogisticRegressionWithLBFGS().setNumClasses(12).run(trainData.rdd());
        
        
        print(testData, model);
        print(testData, model1);
        print(testData, model2);
        print(testData, model3);

        
        if (predict_filename == null)
        {
            return ;
        }
        
        
        System.out.println("");
        System.out.println("");
        System.out.println("Predict Result:");
        
        // 预测新数据方法
        {
            JavaRDD<String> predict_data = sc.textFile(predict_filename);
            JavaRDD<Double> predict_result = predict_data.map(line -> {
                double[] ds = Arrays.stream(line.split(" "))
                        .mapToDouble(Double::parseDouble).toArray();
                double[] simple_ds = (double[])resizeArray(ds, col_count);
                org.apache.spark.mllib.linalg.Vector v = (org.apache.spark.mllib.linalg.Vector)Vectors.dense(simple_ds);
                return model.predict(v);
            });
            predict_result.saveAsTextFile(predict_filename + "_model1_result");
        }
    
        
        {
            JavaRDD<String> predict_data = sc.textFile(predict_filename);
            JavaRDD<Double> predict_result = predict_data.map(line -> {
                double[] ds = Arrays.stream(line.split(" "))
                        .mapToDouble(Double::parseDouble).toArray();
                double[] simple_ds = (double[])resizeArray(ds, col_count);
                org.apache.spark.mllib.linalg.Vector v = (org.apache.spark.mllib.linalg.Vector)Vectors.dense(simple_ds);
                return model1.predict(v);
            });
            predict_result.saveAsTextFile(predict_filename + "_model2_result");
        }
        
        {
            JavaRDD<String> predict_data = sc.textFile(predict_filename);
            JavaRDD<Double> predict_result = predict_data.map(line -> {
                double[] ds = Arrays.stream(line.split(" "))
                        .mapToDouble(Double::parseDouble).toArray();
                double[] simple_ds = (double[])resizeArray(ds, col_count);
                org.apache.spark.mllib.linalg.Vector v = (org.apache.spark.mllib.linalg.Vector)Vectors.dense(simple_ds);
                return model2.predict(v);
            });
            predict_result.saveAsTextFile(predict_filename + "_model3_result");
        }
        
        {
            JavaRDD<String> predict_data = sc.textFile(predict_filename);
            JavaRDD<Double> predict_result = predict_data.map(line -> {
                double[] ds = Arrays.stream(line.split(" "))
                        .mapToDouble(Double::parseDouble).toArray();
                double[] simple_ds = (double[])resizeArray(ds, col_count);
                org.apache.spark.mllib.linalg.Vector v = (org.apache.spark.mllib.linalg.Vector)Vectors.dense(simple_ds);
                return model3.predict(v);
            });
            predict_result.saveAsTextFile(predict_filename + "_model4_result");
        }
    }
    
    
    public static void print(JavaRDD<LabeledPoint> testData, GeneralizedLinearModel model) {
        JavaPairRDD<Double, Double> valuesAndPreds = testData.mapToPair(point -> {
            double prediction = model.predict(point.features()); //用模型预测训练数据
            return new Tuple2<>(point.label(), prediction);
        });

        Double MSE = valuesAndPreds.mapToDouble((Tuple2<Double, Double> t) -> Math.pow(t._1() - t._2(), 2)).mean(); //计算预测值与实际值差值的平方值的均值
        System.out.println(model.getClass().getName() + " training Mean Squared Error = " + MSE);
    }
}

{% endhighlight %}


